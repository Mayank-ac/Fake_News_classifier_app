{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f77680",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100   \n",
    "GLOVE_DIR = \"glove/glove.6B.\"+str(EMBEDDING_DIM)+\"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing Dataset\n",
    "\n",
    "true = pd.read_csv(r'data/True.csv')\n",
    "fake = pd.read_csv(r'data/Fake.csv')\n",
    "\n",
    "true['target'] = 0\n",
    "fake['target'] = 1\n",
    "\n",
    "df = pd.concat([true, fake])\n",
    "df = shuffle(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7832de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def basic_text_cleaning(line_from_column):\n",
    "    tokenized_doc = word_tokenize(line_from_column)\n",
    "    \n",
    "    new_review = []\n",
    "    for token in tokenized_doc:\n",
    "        new_token = regex.sub('', token)\n",
    "        if new_token != '':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    new_term_vector = []\n",
    "    for word in new_review:\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    final_doc = []\n",
    "    for word in new_term_vector:\n",
    "        final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    return ' '.join(final_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140f913",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '“' (U+201C) (1369050451.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf[“clean_text”] = df[“text”].progress_map(basic_text_cleaning)\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '“' (U+201C)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tqdm.pandas()  \n",
    "\n",
    "df['clean_text'] = df['text'].progress_map(basic_text_cleaning)\n",
    "df['clean_title'] = df['title'].progress_map(basic_text_cleaning)\n",
    "\n",
    "df.to_csv(\"models/clean_news.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c0b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‘' (U+2018) (3640869389.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf[‘polarity’] = df[‘clean_text’].progress_map(lambda text: TextBlob(str(text)).sentiment.polarity)\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '‘' (U+2018)\n"
     ]
    }
   ],
   "source": [
    "df['polarity'] = df['clean_text'].progress_map(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "df['text_len'] = df['clean_text'].astype(str).progress_map(len)\n",
    "df['text_word_count'] = df['clean_text'].progress_map(lambda x: len(str(x).split()))\n",
    "df['title_len'] = df['clean_title'].astype(str).progress_map(len)\n",
    "df['title_word_count'] = df['clean_title'].progress_map(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb47e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    length = len(data)\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, length + 1) / length\n",
    "    return x, y\n",
    "\n",
    "def generate_hist_ECDF(data, x_label, title):\n",
    "    x_1, y_1 = ecdf(data=data[df['target'] == 1])\n",
    "    x_0, y_0 = ecdf(data=data[df['target'] == 0])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle('ECDF and histogram plots for col — {} comparison between true and false'.format(title))\n",
    "\n",
    "    ax1.plot(x_1, y_1, marker='.', linestyle='none', label='Fake')\n",
    "    ax1.plot(x_0, y_0, marker='.', linestyle='none', label='True')\n",
    "    ax1.set(xlabel=x_label, ylabel='CDF')\n",
    "\n",
    "    ax2.hist(data[df['target'] == 1], density=True, bins=50, alpha=0.6, label='Fake')\n",
    "    ax2.hist(data[df['target'] == 0], density=True, bins=50, alpha=0.6, label='True')\n",
    "    ax2.set(xlabel=x_label, ylabel='Probability')\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    corpus_fake = corpus[df['target'] == 1].astype(str)\n",
    "    corpus_true = corpus[df['target'] == 0].astype(str)\n",
    "\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus_fake)\n",
    "    bow_fake = vec.transform(corpus_fake)\n",
    "    sum_words = bow_fake.sum(axis=0)\n",
    "    words_freq_fake = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_fake = sorted(words_freq_fake, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus_true)\n",
    "    bow_true = vec.transform(corpus_true)\n",
    "    sum_words = bow_true.sum(axis=0)\n",
    "    words_freq_true = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_true = sorted(words_freq_true, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    df_fake = pd.DataFrame(words_freq_fake[:n], columns=['text', 'count'])\n",
    "    df_true = pd.DataFrame(words_freq_true[:n], columns=['text', 'count'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    ax1.bar(df_fake['text'], df_fake['count'])\n",
    "    ax1.set_xticklabels(df_fake['text'], rotation=45)\n",
    "    ax1.set(xlabel='Top {} most frequent terms for fake news'.format(n), ylabel='Count')\n",
    "\n",
    "    ax2.bar(df_true['text'], df_true['count'])\n",
    "    ax2.set_xticklabels(df_true['text'], rotation=45)\n",
    "    ax2.set(xlabel='Top {} most frequent terms for true news'.format(n), ylabel='Count')\n",
    "\n",
    "    plt.suptitle('Comparison of most frequent terms (fake vs true)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    corpus_fake = corpus[df['target'] == 1].astype(str)\n",
    "    corpus_true = corpus[df['target'] == 0].astype(str)\n",
    "\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus_fake)\n",
    "    bow_fake = vec.transform(corpus_fake)\n",
    "    sum_words = bow_fake.sum(axis=0)\n",
    "    words_freq_fake = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_fake = sorted(words_freq_fake, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus_true)\n",
    "    bow_true = vec.transform(corpus_true)\n",
    "    sum_words = bow_true.sum(axis=0)\n",
    "    words_freq_true = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_true = sorted(words_freq_true, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    df_fake = pd.DataFrame(words_freq_fake[:n], columns=['text', 'count'])\n",
    "    df_true = pd.DataFrame(words_freq_true[:n], columns=['text', 'count'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    ax1.bar(df_fake['text'], df_fake['count'])\n",
    "    ax1.set_xticklabels(df_fake['text'], rotation=45)\n",
    "    ax1.set(xlabel='Top {} bigrams for fake news'.format(n), ylabel='Count')\n",
    "\n",
    "    ax2.bar(df_true['text'], df_true['count'])\n",
    "    ax2.set_xticklabels(df_true['text'], rotation=45)\n",
    "    ax2.set(xlabel='Top {} bigrams for true news'.format(n), ylabel='Count')\n",
    "\n",
    "    plt.suptitle('Comparison of bigrams (fake vs true)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 5, figsize=(24, 8))\n",
    "\n",
    "sns.boxplot(y='polarity', x='target', data=df, orient='v', ax=axes[0], showfliers=False)\n",
    "axes[0].set_title('Polarity', fontsize=17)\n",
    "\n",
    "sns.boxplot(y='text_len', x='target', data=df, orient='v', ax=axes[1], showfliers=False)\n",
    "axes[1].set_title('Length of News characters', fontsize=17)\n",
    "\n",
    "sns.boxplot(y='text_word_count', x='target', data=df, orient='v', ax=axes[2], showfliers=False)\n",
    "axes[2].set_title('Length of News words', fontsize=17)\n",
    "\n",
    "sns.boxplot(y='title_len', x='target', data=df, orient='v', ax=axes[3], showfliers=False)\n",
    "axes[3].set_title('Length of News title characters', fontsize=17)\n",
    "\n",
    "sns.boxplot(y='title_word_count', x='target', data=df, orient='v', ax=axes[4], showfliers=False)\n",
    "axes[4].set_title('Length of News title words', fontsize=17)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b9e59d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_hist_ECDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_hist_ECDF\u001b[49m(data=df[\u001b[33m'\u001b[39m\u001b[33mtext_len\u001b[39m\u001b[33m'\u001b[39m], x_label=\u001b[33m'\u001b[39m\u001b[33mtext_len\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mtext_len\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m generate_hist_ECDF(data=df[\u001b[33m'\u001b[39m\u001b[33mtitle_len\u001b[39m\u001b[33m'\u001b[39m], x_label=\u001b[33m'\u001b[39m\u001b[33mtitle_len\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mtitle_len\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_hist_ECDF' is not defined"
     ]
    }
   ],
   "source": [
    "generate_hist_ECDF(data=df['text_len'], x_label='text_len', title='text_len')\n",
    "generate_hist_ECDF(data=df['title_len'], x_label='title_len', title='title_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_hist_ECDF(data=df[\"polarity\"], x_label=\"polarity\", title=\"polarity\")\n",
    "\n",
    "get_top_n_words(corpus=df[\"clean_text\"], n=10)\n",
    "\n",
    "get_top_n_words(corpus=df[\"clean_title\"], n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d71eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_dummies(df, col_target):\n",
    "    df_y = pd.get_dummies(df[col_target])\n",
    "    df_new = df.join(df_y)\n",
    "    df_new = df_new.drop(col_target, axis=1)\n",
    "    return df_new\n",
    "\n",
    "def prep_features(df, labels, text):\n",
    "    y = df[labels].values\n",
    "    comments_train = df[text]\n",
    "    comments_train = list(comments_train)\n",
    "    return comments_train, y\n",
    "\n",
    "def prep_tokenizer(texts, MAX_NB_WORDS):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    print(\"Tokenizer created — Saving Tokenizer\")\n",
    "\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Tokenizer saved\")\n",
    "\n",
    "def prepare_training_test_data(texts, tokenizer, y):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Vocabulary size:', len(word_index))\n",
    "\n",
    "    print(\"Padding sequences\")\n",
    "\n",
    "    data = pad_sequences(sequences, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = y[indices]\n",
    "\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    x_train = data[:-num_validation_samples]\n",
    "    y_train = labels[:-num_validation_samples]\n",
    "    x_val = data[-num_validation_samples:]\n",
    "    y_val = labels[-num_validation_samples:]\n",
    "\n",
    "    print('Number of entries in each category:')\n",
    "    print('training:', y_train.sum(axis=0))\n",
    "    print('validation:', y_val.sum(axis=0))\n",
    "\n",
    "    print('Tokenized sentences:\\n', data[0])\n",
    "    print('One hot label:\\n', labels[0])\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472a406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75649342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the target labels\n",
    "df_new = get_dummies(df=df, col_target=\"target\")\n",
    "df_new.head()\n",
    "\n",
    "# Generating the X and Y values needed for training\n",
    "labels = [0, 1]\n",
    "x_train, y_train = prep_features(df=df_new, labels=[0, 1], text=\"clean_text\")\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "prep_tokenizer(texts=x_train, MAX_NB_WORDS=MAX_NB_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "x_train, y_train, x_test, y_test = prepare_training_test_data(\n",
    "    texts=x_train,\n",
    "    tokenizer=tokenizer,\n",
    "    y=y_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0499427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "print('Loading GloVe from:', GLOVE_DIR, '…', end='')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "f.close()\n",
    "print(\"Done.\\nProceeding with Embedding Matrix…\", end='')\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"Completed!\")\n",
    "\n",
    "# Create embedding input layer\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=len(word_index) + 1,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False,\n",
    "    name='embeddings'\n",
    ")\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c96134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "preds = Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(loss)+1)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "ax1.plot(epochs, loss, label='Training loss')\n",
    "ax1.plot(epochs, val_loss, label='Validation loss')\n",
    "ax1.set_title('Training and validation loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax2.plot(epochs, acc, label='Training accuracy')\n",
    "ax2.plot(epochs, val_acc, label='Validation accuracy')\n",
    "ax2.set_title('Training and validation accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd304ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/fake_news_lstm_model.h5')\n",
    "print(\"Model saved as fake_news_lstm_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
